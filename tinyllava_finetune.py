# -*- coding: utf-8 -*-
"""TinyLlava Finetune.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nSc2KRwLnChL36XJ6bpIw5SgXDFP7T_r

Fine tune Tiny Llava with LORA
"""

# # ✅ Safe install for PEFT + Transformers compatibility

# install the current multimodal stack
!pip install -U \
  "transformers>=4.45.0" \
  "accelerate>=0.29.0" \
  "peft>=0.11.1" \
  datasets bitsandbytes trl sentencepiece einops

from google.colab import drive
drive.mount('/content/drive')
!mkdir -p /content/tinyllava-lora
!cp -r /content/drive/MyDrive/tinyllava-lora-checkpoints_fixed/* /content/tinyllava-lora/


from transformers import LlavaForConditionalGeneration, AutoProcessor
import torch

merged_path = "tinyllava-lora/merged"

# Load the full merged model (LoRA already merged into base)
model = LlavaForConditionalGeneration.from_pretrained(
    merged_path,
    torch_dtype=torch.float16,
    device_map="cuda",
)

# Load the processor (tokenizer + vision processor)
processor = AutoProcessor.from_pretrained(merged_path)

# # -----------------------------------------------------------
# # ✨ 2. Grab the base Tiny‑LLaVA checkpoint
# # -----------------------------------------------------------
# from huggingface_hub import snapshot_download
# from google.colab import userdata

# hf_token = userdata.get("TinyLlava")  # match secret name
# BASE_MODEL = "bczhou/TinyLLaVA-1.5B"      # 1.5 B params, ~4 GB in 4‑bit
# snapshot_download(repo_id=BASE_MODEL, local_dir="base_ckpt", token=hf_token)

from transformers import LlavaForConditionalGeneration, AutoProcessor
import torch

model_id = "bczhou/tiny-llava-v1-hf"

model = LlavaForConditionalGeneration.from_pretrained(
    model_id,
    torch_dtype=torch.float16,      # or torch.float16
    #device_map="auto",               # puts full‑precision weights on GPU
    device_map="cuda:0",      # one‑GPU mapping
)

processor = AutoProcessor.from_pretrained(model_id)
processor.patch_size = model.config.vision_config.patch_size
print("✓ Tiny‑LLaVA loaded in FP16 on A100")

#@title { vertical-output: true}
import re
#
def checl_trainable_parameters(model):
    print("Checking trainable parameters for permutation_net:")
    for name, param in model.named_parameters():
        print(f"{name}: requires_grad={param.requires_grad}")

def get_target_models(model):
    patterns = [
      re.compile(r".*language_model\.layers\.\d+\.self_attn\.(q|k|v|o)_proj$"),
      re.compile(r".*language_model\.layers\.\d+\.mlp\.(gate|up|down)_proj$")
    ]

    matched = []
    for name, module in model.named_modules():
        if any(p.search(name) for p in patterns):
            matched.append(name)
    return matched


# Allow LoRA to update only projector / language layers
# target_modules = [
#     # "q_proj","k_proj","v_proj","o_proj",         # attention
#     # "gate_proj","up_proj","down_proj"            # FFN
#     # LLaMA backbone attention layers
#     # "layers.[0].self_attn.q_proj",

#     # "language_model.layers.self_attn.q_proj",
#     "model.language_model.layers.*.self_attn.k_proj",
#     # "model.language_model.layers.self_attn.v_proj",
#     # "model.language_model.layers.self_attn.o_proj",
#     # # LLaMA backbone FFN layers
#     # "model.language_model.layers.mlp.gate_proj",
#     # "model.language_model.layers.mlp.up_proj",
#     # "model.language_model.layers.mlp.down_proj",

# ]

# Call this before training
# check_trainable_parameters(model)

target_modules = get_target_models(model)
target_modules

# ╔══════════════════════════════════════════════════════════════════╗
# ║ 4. PEFT LoRA configuration (text‑side only)                      ║
# ╚══════════════════════════════════════════════════════════════════╝
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training


# 2. ***patch the flag***
model.config.is_encoder_decoder = False          # <- crucial line

model = prepare_model_for_kbit_training(model)
lora_cfg = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=target_modules,
)

model = get_peft_model(model, lora_cfg)
model.print_trainable_parameters()
# 2 – double‑check the special token id once
model.config.image_token_index = processor.tokenizer.convert_tokens_to_ids("<image>")

from datasets import load_dataset
from PIL import Image
from IPython.display import display
import torch, traceback

IGNORE = -100                      # loss is not computed on these tokens

def load_unsloth_mini(max_samples=8000, seed=42, num_samples_to_show=3, train_test = "train"):
    ds = load_dataset(
        "unsloth/llava-instruct-mix-vsft-mini",
        split=train_test,
        streaming=False
    ).shuffle(seed=seed)
    print(f"Dataset loaded with {len(ds)} examples")

    processed, skips, samples_to_show = [], 0, []

    for idx, row in enumerate(ds):
        try:
            # ------------- 1. pull texts & image ----------------
            user_msg = next(m["content"] for m in row["messages"] if m["role"] == "user")
            asst_msg = next(m["content"] for m in row["messages"] if m["role"] == "assistant")
            # print(f"User: {user_msg}")
            # print(f"Assistant: {asst_msg}")

            #user_text = " ".join(x["text"] for x in user_msg if x["type"] == "text").strip()
            # user_text = "".join(
            #     f"<image>" if x["type"] == "image"
            #     else x["text"]
            #     for x in user_msg
            # ).strip()
            # print(f"User text: {user_text}")

            text_parts = [p["text"] for p in user_msg if p["type"] == "text" and p["text"]]
            has_image = any(p["type"] == "image" for p in user_msg)

            # Start with <image> if present, then append text parts
            user_text = "<image> " if has_image else ""
            user_text += " ".join(text_parts).strip()
            #print(f"User text: {user_text}")

             # ---- 3. Count <image> tokens from user text ----
            num_image_tokens = user_text.count("<image>")
            if num_image_tokens != 1:
                raise ValueError(f"Skipping row with {num_image_tokens} <image> tokens")

            asst_text = " ".join(x["text"] for x in asst_msg if x["type"] == "text").strip()
            #print("asst_text:", asst_text)
            if not user_text or not asst_text:
                raise ValueError("empty user or assistant text")

            img_idx = next((x["index"] for x in user_msg if x["type"] == "image"), 0)
            image = row["images"][img_idx].convert("RGB")

            # ------------- 2. build full conversation -----------
            prompt = f"{user_text}\nASSISTANT:"
            convo  = f"{prompt} {asst_text} {processor.tokenizer.eos_token}"
            #print(f"Full prompt convo:\n{convo}")

            enc = processor(
                text   = convo,
                images = image,
                return_tensors = "pt",
                truncation=True,
                padding=False,
            )
            print(f"processor tokens: {enc}")

            # ------------- 3. make labels -----------------------
            input_ids      = enc.input_ids[0]          # (seq,)
            attention_mask = enc.attention_mask[0]
            pixel_values   = enc.pixel_values[0]

            # how many tokens belong to the prompt part?
            prefix_len = len(
                processor(
                    text   = prompt,
                    images = image,
                    return_tensors = "pt",
                    add_special_tokens = False
                ).input_ids[0]
            )

            labels = input_ids.clone()
            labels[:prefix_len] = IGNORE               # mask human / system tokens
            #print(f"labels: {labels}")

            # decoded_labels_full = processor.tokenizer.decode(
            #     labels, skip_special_tokens=False, clean_up_tokenization_spaces=False
            # )
            # print(f"Decoded labels (full): {decoded_labels_full}")

            # ------------- 4. store -----------------------------
            processed.append({
                "input_ids":      input_ids,
                "attention_mask": attention_mask,
                "pixel_values":   pixel_values,
                "labels":         labels,
            })

            if len(samples_to_show) < num_samples_to_show:
                samples_to_show.append(
                    dict(prompt=prompt, assistant_text=asst_text, image=image)
                )

            if len(processed) >= max_samples:
                break

        except Exception as e:
            if skips < 10:
                print(f"⚠️ Skipped row {idx}: {e}")
                traceback.print_exc()
            skips += 1
            continue

    # ------------ 5. report & show -----------------------------
    print(f"✅ Prepared {len(processed)} examples, skipped {skips} rows\n")
    for i, s in enumerate(samples_to_show, 1):
        print(f"Sample {i}\nPrompt: {s['prompt']} {s['assistant_text']}")
        display(s["image"])

    return processed

  # Build training list
train_data = load_unsloth_mini(max_samples=8, num_samples_to_show=3)
val_data = load_unsloth_mini(max_samples=1, num_samples_to_show=0, train_test = "test")

import torch

class CustomDataCollator:
    def __init__(self, pad_token_id=0, ignore_index=-100):
        self.pad_token_id = pad_token_id
        self.ignore_index = ignore_index

    def __call__(self, features):
        # ── 1. sanity check ──────────────────────────────────────────────────────
        req = {"input_ids", "attention_mask", "pixel_values", "labels"}
        for i, f in enumerate(features):
            if not req.issubset(f):
                raise ValueError(f"Feature {i} missing keys {req - f.keys()}")

        # ── 2. compute max length once ──────────────────────────────────────────
        max_len = max(f["input_ids"].size(0) for f in features)

        input_ids, attn_masks, labels, pixels = [], [], [], []
        for f in features:
            L   = f["input_ids"].size(0)
            pad = max_len - L

            # right-pad inputs and masks
            input_ids.append(torch.cat([f["input_ids"],
                                        f["input_ids"].new_full((pad,), self.pad_token_id)]))
            attn_masks.append(torch.cat([f["attention_mask"],
                                         f["attention_mask"].new_zeros(pad)]))

            # right-pad labels the same way
            labels.append(torch.cat([f["labels"],
                                     f["labels"].new_full((pad,), self.ignore_index)]))

            pixels.append(f["pixel_values"])

        return {
            "input_ids":      torch.stack(input_ids),
            "attention_mask": torch.stack(attn_masks),
            "labels":         torch.stack(labels),
            "pixel_values":   torch.stack(pixels),
        }

"""### BLEU"""

import torch
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from tqdm import tqdm
from datasets import load_dataset
import nltk
nltk.download('punkt')

def compute_bleu(model, processor, val_data, max_new_tokens=64, do_sample=False):
    model.eval()
    predictions, references = [], []

    for sample in tqdm(val_data, desc="Computing BLEU"):

        # 1 · build prompt (assume exactly one image in this dataset row)
        img   = sample["images"][0]                     # PIL.Image
        parts = sample["messages"][0]["content"]
        # question = " ".join(
        #     "<image>" if p["type"] == "image" else p["text"]
        #     for p in parts
        # ).strip()

        text_parts = [p["text"] for p in parts if p["type"] == "text" and p["text"]]
        has_image = any(p["type"] == "image" for p in parts)

        # Start with <image> if present, then append text parts
        question = "<image> " if has_image else ""
        question += " ".join(text_parts).strip()

        # 2 · ground-truth answer (for reference only)
        gt_answer = next(
            c["text"] for m in sample["messages"]
            if m["role"] == "assistant"
            for c in m["content"] if c["type"] == "text"
        )

        question = question + "\nASSISTANT:"

        # 3 · tokenise + move to GPU
        inputs = processor(
            text   = question,
            images = img,
            return_tensors="pt"
        ).to(model.device)

        gen_ids = model.generate(
            **inputs,
            max_new_tokens = max_new_tokens,
            do_sample      = do_sample,
            eos_token_id   = processor.tokenizer.eos_token_id,
            pad_token_id   = processor.tokenizer.pad_token_id,
        )

        # Keep only the newly generated tokens
        new_tokens = gen_ids[0, inputs.input_ids.shape[1]:]
        generated_text = processor.tokenizer.decode(
            new_tokens, skip_special_tokens=True
        ).lower().strip()

        # Store prediction and reference
        predictions.append(generated_text)
        references.append([gt_answer.lower().strip()])

        # Debug: Print generated vs. reference text
        #print(f"Generated: {generated_text} | Reference: {gt_answer}")

    # Compute BLEU with NLTK
    bleu_scores = []
    for pred, ref in zip(predictions, references):
        score = sentence_bleu(
            references=[r.split() for r in ref],
            hypothesis=pred.split(),
            weights=(0.5, 0.5),  # Unigrams and bigrams, equal weights
            smoothing_function=SmoothingFunction().method1
        )
        bleu_scores.append(score)

    avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0.0
    print(f"\nValidation BLEU Score after LoRA fine-tuning for another epoch: {avg_bleu:.4f}")
    return avg_bleu

# Load validation data (raw dataset, not load_unsloth_mini)
bleu_val_data = load_dataset(
    "unsloth/llava-instruct-mix-vsft-mini",
    split="test",  # Use train split as validation (adjust if needed)
    streaming=False
).shuffle(seed=126).select(range(100))  # Limit to 100 samples

# Compute validation BLEU
torch.cuda.empty_cache()
bleu_score = compute_bleu(
    model=model,
    processor=processor,
    val_data=bleu_val_data,
    max_new_tokens=64,
    do_sample=False
)
print(f"Validation BLEU Score before LORA fine tuning: {bleu_score:.4f}")

"""### BERT Score"""

!pip install bert-score rouge-score evaluate datasets nltk pycocoevalcap

import torch
from bert_score import score
from tqdm import tqdm
import nltk
nltk.download('punkt')

def compute_bertscore(model, processor, val_data, max_new_tokens=64, do_sample=False, temperature=0.7, top_p=0.9):
    model.eval()
    predictions, references = [], []

    for sample in tqdm(val_data, desc="Computing BERTScore"):
        # Mimic compare_infer_results input processing
        img = sample["images"][0]  # Use first image
        parts = sample["messages"][0]["content"]  # User message content
        question = " ".join(
            "<image>" if p["type"] == "image" else p["text"]
            for p in parts
        ).strip()
        gt_answer = next(
            c["text"] for m in sample["messages"]
            if m["role"] == "assistant"
            for c in m["content"] if c["type"] == "text"
        ).strip()

        processor.patch_size = 14
        question = question + "\nASSISTANT:"

        # Prepare input
        inputs = processor(
            text=question,
            images=img,
            return_tensors="pt",
            padding=True,
            truncation=True
        ).to(model.device)

        # Generate answer
        generated_tokens = model.generate(
            pixel_values=inputs["pixel_values"],
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_new_tokens=max_new_tokens,
            do_sample=do_sample
            #temperature=temperature,
            #top_p=top_p
        )

        # Decode generated tokens
        generated_text = processor.tokenizer.decode(generated_tokens[0], skip_special_tokens=True).lower().strip()

        # Store prediction and reference
        predictions.append(generated_text)
        references.append(gt_answer.lower().strip())

    # Compute BERTScore
    P, R, F1 = score(predictions, references, lang="en", model_type="roberta-large", verbose=False)
    avg_bertscore = F1.mean().item()
    print(f"Validation BERTScore (F1): {avg_bertscore:.4f}")
    return avg_bertscore

# Example BERTScore calculation
example_predictions = ["Cat is on mat"]
example_references = ["The cat is sitting on the mat"]
P, R, F1 = score(example_predictions, example_references, lang="en", model_type="roberta-large")
print(f"Example BERTScore (F1): {F1.item():.4f}")  # Expected: ~0.8-0.9 depending on model

# Load validation data
from datasets import load_dataset
bleu_val_data = load_dataset(
    "unsloth/llava-instruct-mix-vsft-mini",
    split="test",
    streaming=False
).shuffle(seed=126).select(range(500))

# Compute validation BERTScore
torch.cuda.empty_cache()
bertscore = compute_bertscore(
    model=model,
    processor=processor,
    val_data=bleu_val_data,
    max_new_tokens=64,
    do_sample=False
)
print(f"Validation BERTScore before LORA fine tuning: {bertscore:.4f}")

"""### Meteor"""

import torch
from evaluate import load
from tqdm import tqdm
import nltk
nltk.download('punkt')
nltk.download('wordnet')  # Required for METEOR

def compute_meteor(model, processor, val_data, max_new_tokens=64, do_sample=False, temperature=0.7, top_p=0.9):
    model.eval()
    predictions, references = [], []

    for sample in tqdm(val_data, desc="Computing METEOR"):
        img = sample["images"][0]
        parts = sample["messages"][0]["content"]
        question = " ".join(
            "<image>" if p["type"] == "image" else p["text"]
            for p in parts
        ).strip()
        gt_answer = next(
            c["text"] for m in sample["messages"]
            if m["role"] == "assistant"
            for c in m["content"] if c["type"] == "text"
        ).strip()

        processor.patch_size = 14
        question = question + "\nASSISTANT:"

        inputs = processor(
            text=question,
            images=img,
            return_tensors="pt",
            padding=True,
            truncation=True
        ).to(model.device)

        generated_tokens = model.generate(
            pixel_values=inputs["pixel_values"],
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_new_tokens=max_new_tokens,
            do_sample=do_sample
        )

        generated_text = processor.tokenizer.decode(generated_tokens[0], skip_special_tokens=True).lower().strip()

        predictions.append(generated_text)
        references.append(gt_answer.lower().strip())

    # Compute METEOR
    meteor = load("meteor")
    meteor_scores = [meteor.compute(predictions=[pred], references=[ref])['meteor'] for pred, ref in zip(predictions, references)]
    avg_meteor = sum(meteor_scores) / len(meteor_scores) if meteor_scores else 0.0
    print(f"Validation METEOR: {avg_meteor:.4f}")
    return avg_meteor

# Example METEOR calculation
example_predictions = ["Cat is on mat"]
example_references = ["The cat is sitting on the mat"]
meteor = load("meteor")
meteor_score = meteor.compute(predictions=example_predictions, references=example_references)['meteor']
print(f"Example METEOR: {meteor_score:.4f}")  # Expected: ~0.3-0.4

# Load validation data
from datasets import load_dataset
bleu_val_data = load_dataset(
    "unsloth/llava-instruct-mix-vsft-mini",
    split="test",
    streaming=False
).shuffle(seed=126).select(range(500))

# Compute validation METEOR
torch.cuda.empty_cache()
meteor_score = compute_meteor(
    model=model,
    processor=processor,
    val_data=bleu_val_data,
    max_new_tokens=64,
    do_sample=False
)
print(f"Validation METEOR before LORA fine tuning: {meteor_score:.4f}")

import torch
from pycocoevalcap.cider.cider import Cider
from tqdm import tqdm
import nltk
nltk.download('punkt')

def compute_cider(model, processor, val_data, max_new_tokens=64, do_sample=False, temperature=0.7, top_p=0.9):
    model.eval()
    predictions, references = [], []

    for sample in tqdm(val_data, desc="Computing CIDEr"):
        img = sample["images"][0]
        parts = sample["messages"][0]["content"]
        question = " ".join(
            "<image>" if p["type"] == "image" else p["text"]
            for p in parts
        ).strip()
        gt_answer = next(
            c["text"] for m in sample["messages"]
            if m["role"] == "assistant"
            for c in m["content"] if c["type"] == "text"
        ).strip()

        processor.patch_size = 14

        question = question + "\nASSISTANT:"

        inputs = processor(
            text=question,
            images=img,
            return_tensors="pt",
            padding=True,
            truncation=True
        ).to(model.device)

        generated_tokens = model.generate(
            pixel_values=inputs["pixel_values"],
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_new_tokens=max_new_tokens,
            do_sample=do_sample
        )

        generated_text = processor.tokenizer.decode(generated_tokens[0], skip_special_tokens=True).lower().strip()

        predictions.append(generated_text)
        references.append([gt_answer.lower().strip()])  # CIDEr expects list of references

    # Format for pycocoevalcap: {image_id: [caption]}
    gts = {i: refs for i, refs in enumerate(references)}
    res = {i: [pred] for i, pred in enumerate(predictions)}

    # Compute CIDEr
    cider_scorer = Cider()
    avg_cider, cider_scores = cider_scorer.compute_score(gts, res)
    print(f"Validation CIDEr: {avg_cider:.4f}")
    return avg_cider

# Example CIDEr calculation
example_predictions = ["Cat is on mat"]
example_references = [["The cat is sitting on the mat"]]
gts = {0: example_references[0]}
res = {0: [example_predictions[0]]}
cider_scorer = Cider()
example_cider, _ = cider_scorer.compute_score(gts, res)
print(f"Example CIDEr: {example_cider:.4f}")  # Expected: ~0.2-0.5 depending on n-gram overlap

# Load validation data
from datasets import load_dataset
bleu_val_data = load_dataset(
    "unsloth/llava-instruct-mix-vsft-mini",
    split="test",
    streaming=False
).shuffle(seed=126).select(range(100))

# Compute validation CIDEr
torch.cuda.empty_cache()
cider_score = compute_cider(
    model=model,
    processor=processor,
    val_data=bleu_val_data,
    max_new_tokens=64,
    do_sample=False
)
print(f"Validation CIDEr before LORA fine tuning: {cider_score:.4f}")

import torch
from rouge_score import rouge_scorer
from tqdm import tqdm
import nltk
nltk.download('punkt')

def compute_rouge_l(model, processor, val_data, max_new_tokens=64, do_sample=False, temperature=0.7, top_p=0.9):
    model.eval()
    predictions, references = [], []

    for sample in tqdm(val_data, desc="Computing ROUGE-L"):
        img = sample["images"][0]
        parts = sample["messages"][0]["content"]
        question = " ".join(
            "<image>" if p["type"] == "image" else p["text"]
            for p in parts
        ).strip()
        gt_answer = next(
            c["text"] for m in sample["messages"]
            if m["role"] == "assistant"
            for c in m["content"] if c["type"] == "text"
        ).strip()

        processor.patch_size = 14
        question = question + "\nASSISTANT:"

        inputs = processor(
            text=question,
            images=img,
            return_tensors="pt",
            padding=True,
            truncation=True
        ).to(model.device)

        generated_tokens = model.generate(
            pixel_values=inputs["pixel_values"],
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_new_tokens=max_new_tokens,
            do_sample=do_sample
        )

        generated_text = processor.tokenizer.decode(generated_tokens[0], skip_special_tokens=True).lower().strip()

        predictions.append(generated_text)
        references.append(gt_answer.lower().strip())

    # Compute ROUGE-L
    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
    rouge_l_scores = [scorer.score(ref, pred)['rougeL'].fmeasure for ref, pred in zip(references, predictions)]
    avg_rouge_l = sum(rouge_l_scores) / len(rouge_l_scores) if rouge_l_scores else 0.0
    print(f"Validation ROUGE-L (F1): {avg_rouge_l:.4f}")
    return avg_rouge_l

# Example ROUGE-L calculation
example_predictions = ["Cat is on mat"]
example_references = ["The cat is sitting on the mat"]
scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
rouge_l = scorer.score(example_references[0], example_predictions[0])['rougeL'].fmeasure
print(f"Example ROUGE-L (F1): {rouge_l:.4f}")  # Expected: ~0.5-0.6

# Load validation data
from datasets import load_dataset
bleu_val_data = load_dataset(
    "unsloth/llava-instruct-mix-vsft-mini",
    split="test",
    streaming=False
).shuffle(seed=126).select(range(100))

# Compute validation ROUGE-L
torch.cuda.empty_cache()
rouge_l_score = compute_rouge_l(
    model=model,
    processor=processor,
    val_data=bleu_val_data,
    max_new_tokens=64,
    do_sample=False
)
print(f"Validation ROUGE-L before LORA fine tuning: {rouge_l_score:.4f}")

"""### Training"""

from transformers import TrainingArguments, Trainer, default_data_collator
import torch
import os
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"
os.environ["TORCH_USE_CUDA_DSA"] = "1"

# Clear GPU memory
torch.cuda.empty_cache()

training_args = TrainingArguments(
    output_dir="tinyllava-lora",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    optim="paged_adamw_8bit",
    num_train_epochs=10,  # Increased for demonstration
    learning_rate=1e-4,
    lr_scheduler_type="cosine",
    warmup_ratio = 0.03,
    fp16=True,
    bf16=False,
    logging_steps=10,
    save_steps=1000,
    save_total_limit=2,
    report_to="none",
    remove_unused_columns=False,
    eval_strategy="epoch",  # Evaluate at the end of each epoch, COMMENT if you don't want to validate or want to see the loss curve
    per_device_eval_batch_size=1, # Evaluate at the end of each epoch, COMMENT if you don't want to validate or want to see the loss curve
)

# Create collator with tokenizer’s pad token ID
collator = CustomDataCollator(pad_token_id=processor.tokenizer.pad_token_id)

# Custom Trainer to compute BLEU score
class CustomTrainer(Trainer):
    def __init__(self, *args, val_data=None, processor=None, **kwargs):
        super().__init__(*args, **kwargs)
        self.val_data = val_data
        self.processor = processor

    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix="eval"):
        # Call the parent evaluate method to compute standard metrics (e.g., loss)
        metrics = super().evaluate(eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)

        # Compute BLEU score on validation data
        if self.val_data is not None:
            bleu_score = compute_bleu(
                model=self.model,
                processor=self.processor,
                val_data=self.val_data,
                max_new_tokens=64,
                do_sample=False
            )
            metrics[f"{metric_key_prefix}_bleu"] = bleu_score
            self.log(metrics)

        return metrics



  # Initialize CustomTrainer
trainer = CustomTrainer(
    model=model,
    args=training_args,
    train_dataset=train_data,
    eval_dataset=val_data[:500],  # Use a subset of training data for evaluation (or replace with val_data)
    data_collator=collator,
    val_data=bleu_val_data,  # Pass the validation dataset
    processor=processor,  # Pass the processor
)

# Train the model
trainer.train()

# ╔══════════════════════════════════════════════════════════════════╗
# ║ 8. Merge LoRA → single file & push / download                    ║
# ╚══════════════════════════════════════════════════════════════════╝
peft_path = "tinyllava-lora/final_checkpoint"
trainer.save_model(peft_path)          # LoRA adapters only

# merge LoRA into the base weights for standalone inference
merged_model = model.merge_and_unload()
merged_path  = "tinyllava-lora/merged"
merged_model.save_pretrained(merged_path)
processor.save_pretrained(merged_path)

print("🚀 All done – fine‑tuned Tiny‑LLaVA saved to:", merged_path)

from google.colab import drive
drive.mount('/content/drive')
!mkdir -p /content/drive/MyDrive/05_29_original_vit_frozen
!cp -r /content/tinyllava-lora/* /content/drive/MyDrive/05_29_original_vit_frozen/

"""---------------------------------------------------------------------------
DIRECTLY START FROM THIS BLOCK IF LOADING MODEL FROM GOOGLE DRIVE!
---------------------------------------------------------------------------
"""

from transformers import LlavaForConditionalGeneration, AutoProcessor
import torch

merged_path = "tinyllava-lora/merged"

# Load the full merged model (LoRA already merged into base)
model = LlavaForConditionalGeneration.from_pretrained(
    merged_path,
    torch_dtype=torch.float16,
    device_map="cuda",
)

# Load the processor (tokenizer + vision processor)
processor = AutoProcessor.from_pretrained(merged_path)
model.eval()

from transformers import AutoProcessor
from transformers import LlavaForConditionalGeneration
import torch

# Load base model with custom class
base_model_id = "bczhou/tiny-llava-v1-hf"
base_model = LlavaForConditionalGeneration.from_pretrained(
    base_model_id,
    torch_dtype=torch.float16,
    device_map="cuda:0"

)
base_processor = AutoProcessor.from_pretrained(base_model_id)
base_processor.patch_size = 14
base_model.eval()
print("✓ Base TinyLLaVA loaded in FP16 on CUDA")

# Compute validation BLEU
torch.cuda.empty_cache()
# bleu_score = compute_bleu(
#     model=base_model,
#     processor=base_processor,
#     val_data=val_data,
#     max_new_tokens=64,
#     do_sample=False
# )
# print(f"Validation BLEU Score before LORA: {bleu_score:.4f}")
#Validation BLEU Score before LORA: 0.0694

from datasets import load_dataset
from IPython.display import display
import torch, matplotlib.pyplot as plt

def show_infer_results(ds, idxes=[0], max_new_tokens=64, do_sample=True):
    model.eval()

    for idx in idxes:
        sample = ds[idx]

        print(sample)

        # 1 · build prompt (assume exactly one image in this dataset row)
        img   = sample["images"][0]                     # PIL.Image
        parts = sample["messages"][0]["content"]
        # question = " ".join(
        #     "<image>" if p["type"] == "image" else p["text"]
        #     for p in parts
        # ).strip()

        text_parts = [p["text"] for p in parts if p["type"] == "text" and p["text"]]
        has_image = any(p["type"] == "image" for p in parts)

        # Start with <image> if present, then append text parts
        question = "<image> " if has_image else ""
        question += " ".join(text_parts).strip()


        # 2 · ground-truth answer (for reference only)
        gt_answer = next(
            c["text"] for m in sample["messages"]
            if m["role"] == "assistant"
            for c in m["content"] if c["type"] == "text"
        )

        prompt = f"{question}\nASSISTANT:"


        # 3 · tokenise + move to GPU
        inputs = processor(
            text   = prompt,
            images = img,
            return_tensors="pt"
        ).to(model.device)

        gen_ids_base = base_model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=do_sample,
            temperature=0.7,
            top_p=0.9,
            eos_token_id=base_processor.tokenizer.eos_token_id,
            pad_token_id=base_processor.tokenizer.pad_token_id,
        )

        new_tokens_base = gen_ids_base[0, inputs.input_ids.shape[1]:]
        answer_base = base_processor.tokenizer.decode(
            new_tokens_base, skip_special_tokens=True
        ).strip()


        # 4 · generation
        gen_ids = model.generate(
            **inputs,
            max_new_tokens = max_new_tokens,
            do_sample      = do_sample,
            temperature    = 0.7,
            top_p          = 0.9,
            eos_token_id   = processor.tokenizer.eos_token_id,
            pad_token_id   = processor.tokenizer.pad_token_id,
        )

        # keep only the newly generated tokens
        new_tokens = gen_ids[0, inputs.input_ids.shape[1]:]
        answer = processor.tokenizer.decode(
            new_tokens, skip_special_tokens=True
        ).strip()

        # --- Display Results ---
        print("═" * 80)
        print(f"Sample {idx}")
        print("Prompt:", question.replace("<image>", "[IMAGE]"))
        print("\nGround-truth:", gt_answer)
        print("Base Model Answer:", answer_base or "<empty>")
        print("Fine-Tuned Model Answer:", answer or "<empty>")
        display(img)

seed = 19
test_ds = load_dataset(
    "unsloth/llava-instruct-mix-vsft-mini",
    split="test",
    streaming=False
).shuffle(seed=seed)

show_infer_results(test_ds, max_new_tokens=128, idxes=list(range(8)))

{
  "messages": [
    {
      "content": [
        {
          "index": null,
          "text": "Who is the author of this book?\n",
          "type": "text"
        },
        {
          "index": 0,
          "text": null,
          "type": "image"
        }
      ],
      "role": "user"
    },
    {
      "content": [
        {
          "index": null,
          "text": "M. H. Abrams",
          "type": "text"
        }
      ],
      "role": "assistant"
    },
    {
      "content": [
        {
          "index": null,
          "text": "What is the title of this book?",
          "type": "text"
        }
      ],
      "role": "user"
    },
    {
      "content": [
        {
          "index": null,
          "text": "The Norton Anthology of English Literature (Ninth Edition)  (Vol. C)",
          "type": "text"
        }
      ],
      "role": "assistant"
    },
    {
      "content": [
        {
          "index": null,
          "text": "What is the genre of this book?",
          "type": "text"
        }
      ],
      "role": "user"
    },
    {
      "content": [
        {
          "index": null,
          "text": "Literature & Fiction",
          "type": "text"
        }
      ],
      "role": "assistant"
    },
    {
      "content": [
        {
          "index": null,
          "text": "Is this book related to Literature & Fiction?",
          "type": "text"
        }
      ],
      "role": "user"
    },
    {
      "content": [
        {
          "index": null,
          "text": "Yes",
          "type": "text"
        }
      ],
      "role": "assistant"
    },
    {
      "content": [
        {
          "index": null,
          "text": "Is this book related to Calendars?",
          "type": "text"
        }
      ],
      "role": "user"
    },
    {
      "content": [
        {
          "index": null,
          "text": "No",
          "type": "text"
        }
      ],
      "role": "assistant"
    }
  ],
  "images": [
    "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=324x500>"
  ]
}