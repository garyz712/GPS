# -*- coding: utf-8 -*-
"""TinyLlava_GumbelSinkhornPermutationNet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kYk9TglMKAX57Zxw2WGYAOBYgyoPfKpV

Fine tune Tiny Llava with LORA

### headers
"""



#@title { vertical-output: true}
# # ✅ Safe install for PEFT + Transformers compatibility

# install the current multimodal stack
!pip install -U \
  "transformers>=4.45.0" \
  "accelerate>=0.29.0" \
  "peft>=0.11.1" \
  datasets bitsandbytes trl sentencepiece einops evaluate

# # -----------------------------------------------------------
# # ✨ 2. Grab the base Tiny‑LLaVA checkpoint
# # -----------------------------------------------------------
# from huggingface_hub import snapshot_download
# from google.colab import userdata

# hf_token = userdata.get("TinyLlava")  # match secret name
# BASE_MODEL = "bczhou/TinyLLaVA-1.5B"      # 1.5 B params, ~4 GB in 4‑bit
# snapshot_download(repo_id=BASE_MODEL, local_dir="base_ckpt", token=hf_token)

today_dir = "05_29"

from google.colab import drive
drive.mount('/content/drive')
# !mkdir -p "/content/{today_dir}"

import torch
import torch.nn as nn
import torch.nn.functional as F

# Gumbel & Sinkhorn Utils (unchanged)
def sample_gumbel(shape, eps=1e-10, device=torch.device("cuda")):
    U = torch.rand(shape, device=device)
    return -torch.log(-torch.log(U + eps) + eps)

def sinkhorn(log_alpha, n_iters=50, clamp_range=(-15, 15), tol=1e-6):
    log_alpha = torch.clamp(log_alpha, clamp_range[0], clamp_range[1])
    alpha = torch.exp(log_alpha)
    for _ in range(n_iters):
        alpha_prev = alpha.clone()
        alpha = alpha / (alpha.sum(dim=-1, keepdim=True) + 1e-8)
        alpha = alpha / (alpha.sum(dim=-2, keepdim=True) + 1e-8)
        if torch.all(torch.abs(alpha - alpha_prev) < tol):
            break
    return alpha

def straight_through_permutation(soft_perm, device):
    with torch.no_grad():
        hard_perm = torch.zeros_like(soft_perm)
        indices = torch.argmax(soft_perm, dim=-1)
        batch_idx = torch.arange(soft_perm.size(0), device=device).unsqueeze(-1)
        row_idx = torch.arange(soft_perm.size(1), device=device).unsqueeze(0)
        hard_perm[batch_idx, row_idx, indices] = 1.0
    return hard_perm + (soft_perm - soft_perm.detach())

# Custom Single-Head Attention (unchanged)
class CustomSelfAttention(nn.Module):
    def __init__(self, d_model, dropout=0.1):
        super().__init__()
        self.d_model = d_model
        self.WQ = nn.Linear(d_model, d_model, bias=False)
        self.WK = nn.Linear(d_model, d_model, bias=False)
        self.WV = nn.Linear(d_model, d_model, bias=False)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        B, n, d = x.shape
        Q = self.WQ(x)
        K = self.WK(x)
        V = self.WV(x)
        scores = torch.bmm(Q, K.transpose(1, 2)) / (d ** 0.5)
        attn_weights = F.softmax(scores, dim=-1)
        attn_weights = self.dropout(attn_weights)
        attn_out = torch.bmm(attn_weights, V)
        return scores, attn_out

# Transformer Block (unchanged)
class TransformerBlock(nn.Module):
    def __init__(self, d_model, dim_feedforward=8192, dropout=0.1):
        super().__init__()
        self.self_attn = CustomSelfAttention(d_model, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.ff = nn.Sequential(
            nn.Linear(d_model, dim_feedforward),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(dim_feedforward, d_model),
        )
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, alpha=0.5):
        B, n, d = x.shape
        scores, attn_out = self.self_attn(x)
        identity = torch.eye(n, device=x.device).unsqueeze(0).repeat(B, 1, 1)
        scores = scores + alpha * identity
        x = self.norm1(x + self.dropout(attn_out))
        ff_out = self.ff(x)
        x = self.norm2(x + self.dropout(ff_out))
        return scores, x

# Gumbel-Sinkhorn Network (modified to accept dynamic n)
class GumbelSinkhornPermutationNet(nn.Module):
    def __init__(self, d=2048, hidden=8192, num_layers=4, sinkhorn_iters=50):
        super().__init__()
        self.d = d
        self.blocks = nn.ModuleList([
            TransformerBlock(d, hidden) for _ in range(num_layers)
        ])
        self.sinkhorn_iters = sinkhorn_iters
        self.apply(self._initialize_weights)  # Apply during init


    def _initialize_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.xavier_uniform_(module.weight)
            if module.bias is not None:
                nn.init.zeros_(module.bias)
        elif isinstance(module, nn.LayerNorm):
            nn.init.ones_(module.weight)
            nn.init.zeros_(module.bias)

    def forward(self, tokens, tau=1.0, alpha=0.5, hard=False):
        B, N, D = tokens.shape
        assert D == self.d, f"Input dimension {D} must match model config {self.d}"
        x = tokens
        scores = None
        for block in self.blocks:
            scores, x = block(x, alpha=alpha)
        gumbel_noise = sample_gumbel(scores.shape, device=tokens.device)
        log_alpha = (scores + gumbel_noise) / tau
        soft_perm = sinkhorn(log_alpha, n_iters=self.sinkhorn_iters)
        if hard:
            return straight_through_permutation(soft_perm, tokens.device)
        return soft_perm

# Modified TokenMixer Class
class TokenMixer(nn.Module):
    def __init__(self, d_model=2048, hidden=8192, num_layers=4, sinkhorn_iters=50):
        super().__init__()
        self.d_model = d_model
        self.hidden = hidden
        self.num_layers = num_layers
        self.sinkhorn_iters = sinkhorn_iters
        self.permutation_net = GumbelSinkhornPermutationNet(
            d=d_model,
            hidden=hidden,
            num_layers=num_layers,
            sinkhorn_iters=sinkhorn_iters
        )

    def forward(
        self,
        pixel_values,
        input_ids,
        attention_mask,
        labels,
        tokenizer,
        language_model_embedding,
        vision_tower,
        projector,
        llama_hidden_size=2048,
        use_permutation=True,
        tau=1.0,
        alpha=0.5,
        hard_permutation=False
    ):
        """
        Reorders input tokens using a learned permutation matrix from GumbelSinkhornPermutationNet.
        Only image patches and prompt text tokens (excluding special tokens) are permuted.
        Sequence length is determined dynamically to avoid truncation.
        Args:
            pixel_values: [batch, 3, H, W]
            input_ids: [batch, seq_len]
            attention_mask: [batch, seq_len]
            labels: [batch, seq_len], with -100 for prompt tokens, or None during inference
            tokenizer: Processor’s tokenizer
            language_model_embedding: Language model’s embedding layer
            vision_tower: CLIP vision encoder
            projector: Multimodal projector
            llama_hidden_size: Hidden size of LLaMA (default: 2048)
            use_permutation: Whether to apply token reordering (default: True)
            tau: Temperature for Gumbel-Sinkhorn (default: 1.0)
            alpha: Identity matrix scaling for attention (default: 0.5)
            hard_permutation: Whether to use hard permutation (default: False)
        Returns:
            inputs_embeds: [batch, new_seq_len, 2048]
            new_attention_mask: [batch, new_seq_len]
            new_labels: [batch, new_seq_len] (None during inference)
        """
        if not use_permutation:
            inputs_embeds = language_model_embedding(input_ids)
            return inputs_embeds, attention_mask, labels

        bsz = input_ids.size(0)
        device = input_ids.device

        # 1. Obtain image patch embeddings
        img_hidden = vision_tower(pixel_values).last_hidden_state[:, 1:, :]  # [B, 576, 1024]
        img_embeds = projector(img_hidden).to(dtype=language_model_embedding.weight.dtype)  # [B, 576, 2048]

        # Pre-compute special token IDs
        image_token_id = tokenizer.convert_tokens_to_ids("<image>")
        sep_id = tokenizer("ASSISTANT:", add_special_tokens=False).input_ids[-1]
        special_token_ids = {image_token_id, sep_id, tokenizer.pad_token_id, tokenizer.eos_token_id}

        reordered_seqs, reordered_atts, reordered_labs = [], [], []

        for b in range(bsz):
            # 2. Create prompt mask
            if labels is not None:
                prompt_mask = labels[b].eq(-100)
            else:
                hits = (input_ids[b] == sep_id).nonzero(as_tuple=True)[0]
                prompt_mask = torch.arange(input_ids.size(1), device=device) <= (hits.max() if hits.numel() > 0 else input_ids.size(1) - 1)

            # Extract prompt text tokens, excluding special tokens
            prompt_ids = input_ids[b][prompt_mask]
            prompt_text = prompt_ids[~torch.isin(prompt_ids, torch.tensor(list(special_token_ids), device=device))]

            # 3. Get text embeddings
            if prompt_text.numel() == 0:  # Image-only prompt
                txt_embeds = torch.zeros((1, llama_hidden_size), device=device, dtype=img_embeds.dtype)
                txt_labels = torch.tensor([-100], device=device, dtype=torch.long)
                num_txt = 1
            else:
                txt_embeds = language_model_embedding(prompt_text.unsqueeze(0)).squeeze(0)  # [T, 2048]
                num_txt = txt_embeds.size(0)
                txt_labels = torch.full((num_txt,), -100, dtype=torch.long, device=device)

            # 4. Concatenate image and text embeddings
            num_img = img_embeds[b].size(0)  # Number of image patches (e.g., 576)
            combined_embeds = torch.cat([img_embeds[b], txt_embeds], dim=0)  # [576 + T, 2048]
            seq_len = combined_embeds.size(0)

            # Apply permutation
            combined_embeds = combined_embeds.unsqueeze(0)  # [1, seq_len, 2048]
            perm_matrix = self.permutation_net(
                combined_embeds,
                tau=tau,
                alpha=alpha,
                hard=hard_permutation
            )  # [1, seq_len, seq_len]
            permuted_embeds = torch.bmm(perm_matrix, combined_embeds)  # [1, seq_len, 2048]
            permuted_embeds = permuted_embeds.squeeze(0)  # [seq_len, 2048]

            # 6. Construct labels and attention mask
            lab = [-100] * num_img + txt_labels.tolist()
            if labels is not None:
                resp_mask = ~prompt_mask
                response_embeds = language_model_embedding(input_ids[b])[resp_mask]
                permuted_embeds = torch.cat([permuted_embeds, response_embeds], dim=0)
                lab.extend(labels[b][resp_mask].tolist())

            att = torch.ones(len(permuted_embeds), device=device, dtype=torch.long)
            lab = torch.tensor(lab, device=device, dtype=torch.long)

            reordered_seqs.append(permuted_embeds)
            reordered_atts.append(att)
            reordered_labs.append(lab)

        # 7. Pad to equal length (as in original code)
        max_len = max(seq.size(0) for seq in reordered_seqs) if reordered_seqs else 1
        dtype = img_embeds.dtype
        pad_seq = torch.zeros(bsz, max_len, llama_hidden_size, device=device, dtype=dtype)
        pad_att = torch.zeros(bsz, max_len, device=device, dtype=torch.long)
        pad_lab = None if labels is None else torch.full(
            (bsz, max_len), -100, device=device, dtype=torch.long
        )

        for b, (seq, att, lab) in enumerate(zip(reordered_seqs, reordered_atts, reordered_labs)):
            if seq.numel() > 0:
                pad_seq[b, :seq.size(0)] = seq
                pad_att[b, :att.size(0)] = att
                if labels is not None:
                    pad_lab[b, :lab.size(0)] = lab

        return pad_seq, pad_att, pad_lab

from transformers import CLIPTextModel, CLIPTokenizer
from transformers import LlavaForConditionalGeneration, AutoProcessor
import torch


class CustomLlavaForConditionalGeneration(LlavaForConditionalGeneration):
    def __init__(self, config):
        super().__init__(config)
        self.token_mixer = TokenMixer()

    def forward(self, input_ids=None, pixel_values=None, attention_mask=None, labels=None, inputs_embeds=None, **kwargs):
        if inputs_embeds is not None:
            # If inputs_embeds is provided (e.g., from validation loop), use it directly
            filtered_kwargs = {k: v for k, v in kwargs.items() if k not in ["input_ids", "pixel_values", "inputs_embeds", "labels"]}
            return super().forward(
                inputs_embeds=inputs_embeds,
                attention_mask=attention_mask,
                labels=labels,
                **filtered_kwargs
            )
        else:
            # Otherwise, compute inputs_embeds using TokenMixer
            embedding_layer = self.get_input_embeddings()
            inputs_embeds, new_attention_mask, new_labels = self.token_mixer(
                pixel_values, input_ids, attention_mask, labels,
                processor.tokenizer, embedding_layer, self.vision_tower, self.multi_modal_projector
            )
            filtered_kwargs = {k: v for k, v in kwargs.items() if k not in ["input_ids", "pixel_values", "inputs_embeds", "labels"]}
            return super().forward(
                inputs_embeds=inputs_embeds,
                attention_mask=new_attention_mask,
                labels=new_labels,
                **filtered_kwargs
            )

    def generate(
        self,
        processor = None,
        input_ids=None,
        pixel_values=None,
        attention_mask=None,
        max_new_tokens=128,
        do_sample=False,
        temperature=1.0,
        top_p=1.0,
        **kwargs
    ):
        """
        Custom generate method for multimodal text generation.

        Args:
            input_ids (torch.LongTensor): Input token IDs.
            pixel_values (torch.FloatTensor, optional): Image pixel values.
            attention_mask (torch.LongTensor, optional): Attention mask for input_ids.
            max_new_tokens (int): Maximum number of new tokens to generate.
            do_sample (bool): Whether to use sampling (True) or greedy decoding (False).
            temperature (float): Temperature for sampling.
            top_p (float): Top-p probability for nucleus sampling.
            **kwargs: Additional arguments passed to the model.

        Returns:
            torch.LongTensor: Generated token IDs.
        """
        self.eval()  # Set model to evaluation mode

        if pixel_values is None:
            # Handle text-only inputs by falling back to parent class
            return super().generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                max_new_tokens=max_new_tokens,
                do_sample=do_sample,
                temperature=temperature,
                top_p=top_p,
                **kwargs
            )

        # Process inputs using token_mixer
        inputs_embeds, attention_mask, _ = self.token_mixer(
            pixel_values=pixel_values,
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=None,
            tokenizer=processor.tokenizer,  # Ensure tokenizer is accessible
            language_model_embedding=self.get_input_embeddings(),
            vision_tower=self.vision_tower,
            projector=self.multi_modal_projector,
            llama_hidden_size=2048
        )

        # Initialize generation
        generated_tokens = []
        current_embeds = inputs_embeds
        current_attention_mask = attention_mask
        past_key_values = None

        # Autoregressive generation loop
        with torch.no_grad():
            for step in range(max_new_tokens):
                # Forward pass
                outputs = self(
                    inputs_embeds=current_embeds[:, -1:, :] if step > 0 else current_embeds,
                    attention_mask=current_attention_mask,
                    past_key_values=past_key_values,
                    use_cache=True,
                    **kwargs
                )
                logits = outputs.logits
                past_key_values = outputs.past_key_values

                # Get logits for the last token
                next_token_logits = logits[:, -1, :]

                # Sampling or greedy decoding
                if do_sample:
                    scaled_logits = next_token_logits / temperature
                    probs = F.softmax(scaled_logits, dim=-1)
                    sorted_probs, sorted_indices = torch.sort(probs, descending=True)
                    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
                    sorted_indices_to_remove = cumulative_probs > top_p
                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                    sorted_indices_to_remove[..., 0] = False
                    probs[sorted_indices_to_remove] = 0
                    probs = probs / probs.sum(dim=-1, keepdim=True)
                    next_token = torch.multinomial(probs, num_samples=1)
                else:
                    next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)

                # Append generated token
                generated_tokens.append(next_token.item())
                if next_token.item() == processor.tokenizer.eos_token_id:
                    break

                # Update inputs for next step
                next_token_embed = self.get_input_embeddings()(next_token)
                current_embeds = torch.cat([current_embeds, next_token_embed], dim=1)
                current_attention_mask = torch.cat(
                    [current_attention_mask, torch.ones_like(next_token, dtype=torch.long, device=self.device)],
                    dim=1
                )

        # Return generated token IDs
        return torch.tensor(generated_tokens, dtype=torch.long, device=self.device)

"""### To train from beginning"""

model_id = "bczhou/tiny-llava-v1-hf"

# Load the modified model
model = CustomLlavaForConditionalGeneration.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    device_map="cuda:0"
)


processor = AutoProcessor.from_pretrained(model_id)
processor.patch_size = model.config.vision_config.patch_size
print("✓ Tiny‑LLaVA loaded in FP16 on A100")

"""### To train from checkpoint"""

load_data = "05_25"

# load pretrained from drive
!mkdir -p "/content/tinyllava-lora/{load_data}/merged"
!cp -r "/content/drive/MyDrive/{load_data}/{load_data}/merged"  \
"/content/tinyllava-lora/{load_data}"

#@title { vertical-output: true}
from transformers import  AutoProcessor
import torch

pretained_path = "tinyllava-lora/"+load_data+"/merged"

# Load the full merged model (LoRA already merged into base)
model = CustomLlavaForConditionalGeneration.from_pretrained(
    pretained_path,
    torch_dtype=torch.float16,
    device_map="cuda",
)

processor = AutoProcessor.from_pretrained(pretained_path)
processor.patch_size = model.config.vision_config.patch_size

# print(model.config.vision_config.patch_size)

"""### Start trainning"""

# ╔══════════════════════════════════════════════════════════════════╗
# ║ 4. PEFT LoRA configuration (text‑side only)                      ║
# ╚══════════════════════════════════════════════════════════════════╝
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

# Allow LoRA to update only projector / language layers
target_modules = [
    "q_proj","k_proj","v_proj","o_proj",         # attention (ViT+LLM)
    "gate_proj","up_proj","down_proj",            # FFN (LLM)
    "mlp.fc1", "mlp.fc2",# FFN (ViT)
    # Multimodal projector (verify names, assuming linear_1 and linear_2)
    "multi_modal_projector.linear_1",
    "multi_modal_projector.linear_2",

]

# 2. ***patch the flag***
model.config.is_encoder_decoder = False          # <- crucial line

model = prepare_model_for_kbit_training(model)
lora_cfg = LoraConfig(
    r=8, # TODO
    lora_alpha=16, # TODO
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=target_modules,
)


model = get_peft_model(model, lora_cfg)

for name, param in model.named_parameters():
    if "token_mixer.permutation_net" in name:
        param.requires_grad = True
    #print(name)
model.print_trainable_parameters()
# 2 – double‑check the special token id once
model.config.image_token_index = processor.tokenizer.convert_tokens_to_ids("<image>")

def check_trainable_parameters(model):
    print("Checking trainable parameters for permutation_net:")
    for name, param in model.named_parameters():
        print(f"{name}: requires_grad={param.requires_grad}")

# Call this before training
check_trainable_parameters(model)

# # ╔══════════════════════════════════════════════════════════════════╗
# # ║ 5. Tiny training split (≈1 000 rows) of LLaVA‑Instruct‑150K      ║
# # ╚══════════════════════════════════════════════════════════════════╝
# from datasets import load_dataset
# from PIL import Image
# from io import BytesIO
# import requests

# ds = load_dataset(
#     "liuhaotian/LLaVA-Instruct-150K",
#     split="train",   # train[:1%] does not work
#     streaming=True
# ).shuffle(buffer_size=1000)

# # quick sanity
# sample = next(iter(ds))
# print(sample.keys(), "\n", sample["conversations"][0])

#@title { vertical-output: true}
from datasets import load_dataset
from PIL import Image
from IPython.display import display
import torch, traceback

IGNORE = -100                      # loss is not computed on these tokens

def load_unsloth_mini(max_samples=8000, seed=42, num_samples_to_show=3, train_test = 'train'):
    ds = load_dataset(
        "unsloth/llava-instruct-mix-vsft-mini",
        split=train_test,
        streaming=False
    ).shuffle(seed=seed)
    print(f"Dataset loaded with {len(ds)} examples")

    processed, skips, samples_to_show = [], 0, []

    for idx, row in enumerate(ds):
        try:
            # ------------- 1. pull texts & image ----------------
            user_msg = next(m["content"] for m in row["messages"] if m["role"] == "user")
            asst_msg = next(m["content"] for m in row["messages"] if m["role"] == "assistant")

            text_parts = [p["text"] for p in user_msg if p["type"] == "text" and p["text"]]
            has_image = any(p["type"] == "image" for p in user_msg)

            # Start with <image> if present, then append text parts
            user_text = "<image> " if has_image else ""
            user_text += " ".join(text_parts).strip()
            #print(f"User text: {user_text}")

             # ---- 3. Count <image> tokens from user text ----
            num_image_tokens = user_text.count("<image>")
            if num_image_tokens != 1:
                raise ValueError(f"Skipping row with {num_image_tokens} <image> tokens")

            asst_text = " ".join(x["text"] for x in asst_msg if x["type"] == "text").strip()
            if not user_text or not asst_text:
                raise ValueError("empty user or assistant text")

            img_idx = next((x["index"] for x in user_msg if x["type"] == "image"), 0)
            image = row["images"][img_idx].convert("RGB")

            # ------------- 2. build full conversation -----------
            prompt = f"{user_text}\nASSISTANT:"
            convo  = f"{prompt} {asst_text} {processor.tokenizer.eos_token}"

            enc = processor(
                text   = convo,
                images = image,
                return_tensors = "pt",
                truncation=True,
                padding=False,
            )

            # ------------- 3. make labels -----------------------
            input_ids      = enc.input_ids[0]          # (seq,)
            attention_mask = enc.attention_mask[0]
            pixel_values   = enc.pixel_values[0]

            # how many tokens belong to the prompt part?
            prefix_len = len(
                processor(
                    text   = prompt,
                    images = image,
                    return_tensors = "pt",
                    add_special_tokens = False
                ).input_ids[0]
            )

            labels = input_ids.clone()
            labels[:prefix_len] = IGNORE               # mask human / system tokens

            # ------------- 4. store -----------------------------
            processed.append({
                "input_ids":      input_ids,
                "attention_mask": attention_mask,
                "pixel_values":   pixel_values,
                "labels":         labels,
            })

            if len(samples_to_show) < num_samples_to_show:
                samples_to_show.append(
                    dict(prompt=prompt, assistant_text=asst_text, image=image)
                )

            if len(processed) >= max_samples:
                break

        except Exception as e:
            if skips < 10:
                print(f"⚠️ Skipped row {idx}: {e}")
                traceback.print_exc()
            skips += 1
            continue

    # ------------ 5. report & show -----------------------------
    print(f"✅ Prepared {len(processed)} examples, skipped {skips} rows\n")
    for i, s in enumerate(samples_to_show, 1):
        print(f"Sample {i}\nPrompt: {s['prompt']} {s['assistant_text']}")
        display(s["image"])

    return processed

  # Build training list
train_data = load_unsloth_mini(max_samples=8000, num_samples_to_show=3)
# Load validation data (test split, as in show_infer_results)
print('*'*20)
train_eval_data = load_unsloth_mini(max_samples=1000, num_samples_to_show=2, train_test='test')  # Limit to 500 samples

import torch

class CustomDataCollator:
    def __init__(self, pad_token_id=0, ignore_index=-100):
        self.pad_token_id = pad_token_id
        self.ignore_index = ignore_index

    def __call__(self, features):
        # ── 1. sanity check ──────────────────────────────────────────────────────
        req = {"input_ids", "attention_mask", "pixel_values", "labels"}
        for i, f in enumerate(features):
            if not req.issubset(f):
                raise ValueError(f"Feature {i} missing keys {req - f.keys()}")

        # ── 2. compute max length once ──────────────────────────────────────────
        max_len = max(f["input_ids"].size(0) for f in features)

        input_ids, attn_masks, labels, pixels = [], [], [], []
        for f in features:
            L   = f["input_ids"].size(0)
            pad = max_len - L

            # right-pad inputs and masks
            input_ids.append(torch.cat([f["input_ids"],
                                        f["input_ids"].new_full((pad,), self.pad_token_id)]))
            attn_masks.append(torch.cat([f["attention_mask"],
                                         f["attention_mask"].new_zeros(pad)]))

            # right-pad labels the same way
            labels.append(torch.cat([f["labels"],
                                     f["labels"].new_full((pad,), self.ignore_index)]))

            pixels.append(f["pixel_values"])

        return {
            "input_ids":      torch.stack(input_ids),
            "attention_mask": torch.stack(attn_masks),
            "labels":         torch.stack(labels),
            "pixel_values":   torch.stack(pixels),
        }

"""### BLEU Score"""

# Use BLEU to Measure Validation loss
import torch
import torch.nn.functional as F
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from tqdm import tqdm
import nltk
nltk.download('punkt')

def compute_bleu(model, processor, val_data, max_new_tokens=64, do_sample=False, temperature=0.7, top_p=0.9):
    model.eval()
    predictions, references = [], []

    for sample in tqdm(val_data, desc="Computing BLEU"):
        # Mimic compare_infer_results input processing
        img = sample["images"][0]  # Use first image, as in compare_infer_results
        parts = sample["messages"][0]["content"]  # User message content

        text_parts = [p["text"] for p in parts if p["type"] == "text" and p["text"]]
        has_image = any(p["type"] == "image" for p in parts)

        # Start with <image> if present, then append text parts
        question = "<image> " if has_image else ""
        question += " ".join(text_parts).strip()

        gt_answer = next(
            c["text"] for m in sample["messages"]
            if m["role"] == "assistant"
            for c in m["content"] if c["type"] == "text"
        ).strip()

        processor.patch_size = 14

        question = question + "\nASSISTANT:"

        # Prepare input
        inputs = processor(
            text=question,
            images=img,
            return_tensors="pt",
            padding=True,
            truncation=True
        ).to(model.device)

        generated_tokens = model.generate(
            processor = processor,
            pixel_values=inputs["pixel_values"],
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_new_tokens=max_new_tokens
        )

        # Decode generated tokens
        generated_text = processor.tokenizer.decode(generated_tokens, skip_special_tokens=True).lower().strip()

        # Store prediction and reference
        predictions.append(generated_text)
        references.append([gt_answer.lower().strip()])

        # Debug: Print generated vs. reference text
        #print(f"Generated: {generated_text} | Reference: {gt_answer}")

    # Compute BLEU with NLTK
    bleu_scores = []
    for pred, ref in zip(predictions, references):
        score = sentence_bleu(
            references=[r.split() for r in ref],
            hypothesis=pred.split(),
            weights=(0.5, 0.5),  # Unigrams and bigrams, equal weights
            smoothing_function=SmoothingFunction().method1
        )
        bleu_scores.append(score)

    avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0.0
    print(f"Current validation BLEU Score: {avg_bleu:.4f}")
    return avg_bleu

# Example BLEU calculation
example_predictions = ["Cat is on mat"]
example_references = [["The cat is sitting on the mat"]]
example_predictions = [text.lower() for text in example_predictions]
example_references = [[ref.lower() for ref in refs] for refs in example_references]
nltk_bleu = sentence_bleu(
    references=[ref[0].split() for ref in example_references],
    hypothesis=example_predictions[0].split(),
    weights=(0.5, 0.5),
    smoothing_function=SmoothingFunction().method1
)
print(f"Example BLEU Score: {nltk_bleu:.4f}")  # Should be ~0.2727

# Load validation data (raw dataset, not load_unsloth_mini)
bleu_val_data = load_dataset(
    "unsloth/llava-instruct-mix-vsft-mini",
    split="test",  # Use train split as validation (adjust if needed)
    streaming=False
).shuffle(seed=126).select(range(100))  # Limit to 100 samples


# # Compute validation BLEU
# torch.cuda.empty_cache()
# bleu_score = compute_bleu(
#     model=model,
#     processor=processor,
#     val_data=bleu_val_data,
#     max_new_tokens=64,
#     do_sample=False
# )
# print(f"Validation BLEU Score before LORA fine tuning: {bleu_score:.4f}")

"""### BertScore"""

!pip install bert-score rouge-score evaluate datasets nltk pycocoevalcap

import torch
from bert_score import score
from tqdm import tqdm
import nltk
nltk.download('punkt')

def compute_bertscore(model, processor, val_data, max_new_tokens=64, do_sample=False, temperature=0.7, top_p=0.9):
    model.eval()
    predictions, references = [], []

    for sample in tqdm(val_data, desc="Computing BERTScore"):
        # Mimic compare_infer_results input processing
        img = sample["images"][0]  # Use first image
        parts = sample["messages"][0]["content"]  # User message content
        question = " ".join(
            "<image>" if p["type"] == "image" else p["text"]
            for p in parts
        ).strip()
        gt_answer = next(
            c["text"] for m in sample["messages"]
            if m["role"] == "assistant"
            for c in m["content"] if c["type"] == "text"
        ).strip()

        processor.patch_size = 14

        question = question + "\nASSISTANT:"

        # Prepare input
        inputs = processor(
            text=question,
            images=img,
            return_tensors="pt",
            padding=True,
            truncation=True
        ).to(model.device)

        # Generate answer
        generated_tokens = model.generate(
            processor=processor,
            pixel_values=inputs["pixel_values"],
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_new_tokens=max_new_tokens,
            do_sample=do_sample,
            temperature=temperature,
            top_p=top_p
        )

        # Decode generated tokens
        generated_text = processor.tokenizer.decode(generated_tokens[0], skip_special_tokens=True).lower().strip()

        # Store prediction and reference
        predictions.append(generated_text)
        references.append(gt_answer.lower().strip())

    # Compute BERTScore
    P, R, F1 = score(predictions, references, lang="en", model_type="roberta-large", verbose=False)
    avg_bertscore = F1.mean().item()
    print(f"Validation BERTScore (F1): {avg_bertscore:.4f}")
    return avg_bertscore

# Example BERTScore calculation
example_predictions = ["Cat is on mat"]
example_references = ["The cat is sitting on the mat"]
P, R, F1 = score(example_predictions, example_references, lang="en", model_type="roberta-large")
print(f"Example BERTScore (F1): {F1.item():.4f}")  # Expected: ~0.8-0.9 depending on model

# Load validation data
from datasets import load_dataset
bleu_val_data = load_dataset(
    "unsloth/llava-instruct-mix-vsft-mini",
    split="test",
    streaming=False
).shuffle(seed=126).select(range(500))

# Compute validation BERTScore
torch.cuda.empty_cache()
bertscore = compute_bertscore(
    model=model,
    processor=processor,
    val_data=bleu_val_data,
    max_new_tokens=64,
    do_sample=False
)
print(f"Validation BERTScore before LORA fine tuning: {bertscore:.4f}")

import torch
from evaluate import load
from tqdm import tqdm
import nltk
nltk.download('punkt')
nltk.download('wordnet')  # Required for METEOR

def compute_meteor(model, processor, val_data, max_new_tokens=64, do_sample=False, temperature=0.7, top_p=0.9):
    model.eval()
    predictions, references = [], []

    for sample in tqdm(val_data, desc="Computing METEOR"):
        img = sample["images"][0]
        parts = sample["messages"][0]["content"]
        question = " ".join(
            "<image>" if p["type"] == "image" else p["text"]
            for p in parts
        ).strip()
        gt_answer = next(
            c["text"] for m in sample["messages"]
            if m["role"] == "assistant"
            for c in m["content"] if c["type"] == "text"
        ).strip()

        processor.patch_size = 14

        question = question + "\nASSISTANT:"

        inputs = processor(
            text=question,
            images=img,
            return_tensors="pt",
            padding=True,
            truncation=True
        ).to(model.device)

        generated_tokens = model.generate(
            processor=processor,
            pixel_values=inputs["pixel_values"],
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_new_tokens=max_new_tokens,
            do_sample=do_sample,
            temperature=temperature,
            top_p=top_p
        )

        generated_text = processor.tokenizer.decode(generated_tokens[0], skip_special_tokens=True).lower().strip()

        predictions.append(generated_text)
        references.append(gt_answer.lower().strip())

    # Compute METEOR
    meteor = load("meteor")
    meteor_scores = [meteor.compute(predictions=[pred], references=[ref])['meteor'] for pred, ref in zip(predictions, references)]
    avg_meteor = sum(meteor_scores) / len(meteor_scores) if meteor_scores else 0.0
    print(f"Validation METEOR: {avg_meteor:.4f}")
    return avg_meteor

# Example METEOR calculation
example_predictions = ["Cat is on mat"]
example_references = ["The cat is sitting on the mat"]
meteor = load("meteor")
meteor_score = meteor.compute(predictions=example_predictions, references=example_references)['meteor']
print(f"Example METEOR: {meteor_score:.4f}")  # Expected: ~0.3-0.4

# Load validation data
from datasets import load_dataset
bleu_val_data = load_dataset(
    "unsloth/llava-instruct-mix-vsft-mini",
    split="test",
    streaming=False
).shuffle(seed=126).select(range(500))

# Compute validation METEOR
torch.cuda.empty_cache()
meteor_score = compute_meteor(
    model=model,
    processor=processor,
    val_data=bleu_val_data,
    max_new_tokens=64,
    do_sample=False
)
print(f"Validation METEOR before LORA fine tuning: {meteor_score:.4f}")

import torch
from pycocoevalcap.cider.cider import Cider
from tqdm import tqdm
import nltk
nltk.download('punkt')

def compute_cider(model, processor, val_data, max_new_tokens=64, do_sample=False, temperature=0.7, top_p=0.9):
    model.eval()
    predictions, references = [], []

    for sample in tqdm(val_data, desc="Computing CIDEr"):
        img = sample["images"][0]
        parts = sample["messages"][0]["content"]
        question = " ".join(
            "<image>" if p["type"] == "image" else p["text"]
            for p in parts
        ).strip()
        gt_answer = next(
            c["text"] for m in sample["messages"]
            if m["role"] == "assistant"
            for c in m["content"] if c["type"] == "text"
        ).strip()

        processor.patch_size = 14

        question = question + "\nASSISTANT:"

        inputs = processor(
            text=question,
            images=img,
            return_tensors="pt",
            padding=True,
            truncation=True
        ).to(model.device)

        generated_tokens = model.generate(
            processor=processor,
            pixel_values=inputs["pixel_values"],
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_new_tokens=max_new_tokens,
            do_sample=do_sample,
            temperature=temperature,
            top_p=top_p
        )

        generated_text = processor.tokenizer.decode(generated_tokens[0], skip_special_tokens=True).lower().strip()

        predictions.append(generated_text)
        references.append([gt_answer.lower().strip()])  # CIDEr expects list of references

    # Format for pycocoevalcap: {image_id: [caption]}
    gts = {i: refs for i, refs in enumerate(references)}
    res = {i: [pred] for i, pred in enumerate(predictions)}

    # Compute CIDEr
    cider_scorer = Cider()
    avg_cider, cider_scores = cider_scorer.compute_score(gts, res)
    print(f"Validation CIDEr: {avg_cider:.4f}")
    return avg_cider

# Example CIDEr calculation
example_predictions = ["Cat is on mat"]
example_references = [["The cat is sitting on the mat"]]
gts = {0: example_references[0]}
res = {0: [example_predictions[0]]}
cider_scorer = Cider()
example_cider, _ = cider_scorer.compute_score(gts, res)
print(f"Example CIDEr: {example_cider:.4f}")  # Expected: ~0.2-0.5 depending on n-gram overlap

# Load validation data
from datasets import load_dataset
bleu_val_data = load_dataset(
    "unsloth/llava-instruct-mix-vsft-mini",
    split="train",
    streaming=False
).shuffle(seed=126).select(range(100))

# Compute validation CIDEr
torch.cuda.empty_cache()
cider_score = compute_cider(
    model=model,
    processor=processor,
    val_data=bleu_val_data,
    max_new_tokens=64,
    do_sample=False
)
print(f"Validation CIDEr before LORA fine tuning: {cider_score:.4f}")

import torch
from rouge_score import rouge_scorer
from tqdm import tqdm
import nltk
nltk.download('punkt')

def compute_rouge_l(model, processor, val_data, max_new_tokens=64, do_sample=False, temperature=0.7, top_p=0.9):
    model.eval()
    predictions, references = [], []

    for sample in tqdm(val_data, desc="Computing ROUGE-L"):
        img = sample["images"][0]
        parts = sample["messages"][0]["content"]
        question = " ".join(
            "<image>" if p["type"] == "image" else p["text"]
            for p in parts
        ).strip()
        gt_answer = next(
            c["text"] for m in sample["messages"]
            if m["role"] == "assistant"
            for c in m["content"] if c["type"] == "text"
        ).strip()

        processor.patch_size = 14
        question = question + "\nASSISTANT:"

        inputs = processor(
            text=question,
            images=img,
            return_tensors="pt",
            padding=True,
            truncation=True
        ).to(model.device)

        generated_tokens = model.generate(
            processor=processor,
            pixel_values=inputs["pixel_values"],
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_new_tokens=max_new_tokens,
            do_sample=do_sample,
            temperature=temperature,
            top_p=top_p
        )

        generated_text = processor.tokenizer.decode(generated_tokens[0], skip_special_tokens=True).lower().strip()

        predictions.append(generated_text)
        references.append(gt_answer.lower().strip())

    # Compute ROUGE-L
    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
    rouge_l_scores = [scorer.score(ref, pred)['rougeL'].fmeasure for ref, pred in zip(references, predictions)]
    avg_rouge_l = sum(rouge_l_scores) / len(rouge_l_scores) if rouge_l_scores else 0.0
    print(f"Validation ROUGE-L (F1): {avg_rouge_l:.4f}")
    return avg_rouge_l

# Example ROUGE-L calculation
example_predictions = ["Cat is on mat"]
example_references = ["The cat is sitting on the mat"]
scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
rouge_l = scorer.score(example_references[0], example_predictions[0])['rougeL'].fmeasure
print(f"Example ROUGE-L (F1): {rouge_l:.4f}")  # Expected: ~0.5-0.6

# Load validation data
from datasets import load_dataset
bleu_val_data = load_dataset(
    "unsloth/llava-instruct-mix-vsft-mini",
    split="test",
    streaming=False
).shuffle(seed=126).select(range(100))

# Compute validation ROUGE-L
torch.cuda.empty_cache()
rouge_l_score = compute_rouge_l(
    model=model,
    processor=processor,
    val_data=bleu_val_data,
    max_new_tokens=64,
    do_sample=False
)
print(f"Validation ROUGE-L before LORA fine tuning: {rouge_l_score:.4f}")

"""###Training"""

from transformers import TrainingArguments, Trainer, default_data_collator
import torch
import os
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"
os.environ["TORCH_USE_CUDA_DSA"] = "1"

# Clear GPU memory
torch.cuda.empty_cache()

training_args = TrainingArguments(
    output_dir="tinyllava-lora/"+today_dir,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    optim="paged_adamw_8bit",
    num_train_epochs=10,  # Increased for demonstration
    learning_rate=1e-4,
    lr_scheduler_type="cosine",
    warmup_ratio = 0.03,
    fp16=True,
    bf16=False,
    logging_steps=10,
    save_steps=1000,
    save_total_limit=None,
    report_to="none",
    remove_unused_columns=False,
    eval_strategy="epoch",  # Evaluate at the end of each epoch, COMMENT if you don't want to validate or want to see the loss curve
    per_device_eval_batch_size=1, # Evaluate at the end of each epoch, COMMENT if you don't want to validate or want to see the loss curve
)

# Create collator with tokenizer’s pad token ID
collator = CustomDataCollator(pad_token_id=processor.tokenizer.pad_token_id)

# Custom Trainer to compute BLEU score
class CustomTrainer(Trainer):
    def __init__(self, *args, val_data=None, processor=None, **kwargs):
        super().__init__(*args, **kwargs)
        self.val_data = val_data
        self.processor = processor

    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix="eval"):
        # Call the parent evaluate method to compute standard metrics (e.g., loss)
        metrics = super().evaluate(eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)

        # Compute BLEU score on validation data
        if self.val_data is not None:
            bleu_score = compute_bleu(
                model=self.model,
                processor=self.processor,
                val_data=self.val_data,
                max_new_tokens=64,
                do_sample=False
            )
            metrics[f"{metric_key_prefix}_bleu"] = bleu_score
            self.log(metrics)

        return metrics


  # Initialize CustomTrainer
trainer = CustomTrainer(
    model=model,
    args=training_args,
    train_dataset=train_data,
    eval_dataset=train_eval_data[:500],  # Use a subset of training data for evaluation (or replace with val_data)
    data_collator=collator,
    val_data=bleu_val_data,  # Pass the validation dataset
    processor=processor,  # Pass the processor
)

# Train the model
trainer.train()

"""### Save"""

# ╔══════════════════════════════════════════════════════════════════╗
# ║ 8. Merge LoRA → single file & push / download                    ║
# ╚══════════════════════════════════════════════════════════════════╝
peft_path = "tinyllava-lora/" + today_dir
trainer.save_model(peft_path)          # LoRA adapters only

# merge LoRA into the base weights for standalone inference
merged_model = model.merge_and_unload()
merged_path  = "tinyllava-lora/"+today_dir+"/merged"
merged_model.save_pretrained(merged_path)
processor.save_pretrained(merged_path)

print("🚀 All done – fine‑tuned Tiny‑LLaVA saved to:", merged_path)

#@title { vertical-output: true}
from transformers import LlavaForConditionalGeneration, AutoProcessor
import torch

merged_path = "tinyllava-lora/"+today_dir+"/merged"

# Load the full merged model (LoRA already merged into base)
model = CustomLlavaForConditionalGeneration.from_pretrained(
    merged_path,
    torch_dtype=torch.float16,
    device_map="cuda",
)

# Load the processor (tokenizer + vision processor)
processor = AutoProcessor.from_pretrained(merged_path)
model.eval()

"""### DIRECTLY START FROM THIS BLOCK IF LOADING MODEL FROM GOOGLE DRIVE!

### Evaluation
"""

# import open_clip, torch

# # 1. reload the (still-frozen) CLIP tower you used during fine-tune
# clip_model, _, _ = open_clip.create_model_and_transforms(
#         'ViT-L-14', pretrained='openai', device='cuda')
# clip_tokenizer = open_clip.get_tokenizer('ViT-L-14')
# clip_model.eval();  clip_model.requires_grad_(False)

# # 2. instantiate the custom model class, *not* LlavaForConditionalGeneration
# model = CustomLlavaForConditionalGeneration.from_pretrained(
#         "tinyllava-lora/merged",          # weights with LoRA already merged
#         # clip_model=clip_model,
#         # clip_tokenizer=clip_tokenizer,
#         torch_dtype=torch.float16,
#         device_map="cuda",
# )

# # 3. processor loads as usual
# from transformers import AutoProcessor
# processor = AutoProcessor.from_pretrained("tinyllava-lora/merged")

# model.eval()

from transformers import AutoProcessor
from transformers import LlavaForConditionalGeneration
import torch

# Load base model with custom class
base_model_id = "bczhou/tiny-llava-v1-hf"
base_model = CustomLlavaForConditionalGeneration.from_pretrained(
    base_model_id,
    torch_dtype=torch.float16,
    device_map="cuda:0"
)
base_processor = AutoProcessor.from_pretrained(base_model_id)
base_model.eval()
print("✓ Base TinyLLaVA loaded in FP16 on CUDA")

from torch.utils.data import DataLoader
from tqdm import tqdm
import torch.nn.functional as F
from IPython.display import display
from datasets import load_dataset

def compare_infer_results(
    ds,
    base_model,
    base_processor,
    fine_tuned_model,
    fine_tuned_processor,
    idxes=[0],
    max_new_tokens=64,
    do_sample=False,
    temperature=0.7,
    top_p=0.9
):
    base_model.eval()
    fine_tuned_model.eval()

    for idx in idxes:
        sample = ds[idx]
        img = sample["images"][0]
        parts = sample["messages"][0]["content"]

        text_parts = [p["text"] for p in parts if p["type"] == "text" and p["text"]]
        has_image = any(p["type"] == "image" for p in parts)

        # Start with <image> if present, then append text parts
        question = "<image> " if has_image else ""
        question += " ".join(text_parts).strip()

        gt_answer = next(
            c["text"] for m in sample["messages"]
            if m["role"] == "assistant"
            for c in m["content"] if c["type"] == "text"
        )
        base_processor.patch_size = 14
        prompt = question + "\nASSISTANT:"

        # --- Base Model Inference ---
        inputs_base = base_processor(
            text=prompt,
            images=img,
            return_tensors="pt",
            padding=True,
            truncation=True
        ).to(base_model.device)

        generated_tokens_base = base_model.generate(
            processor = base_processor,
            pixel_values=inputs_base["pixel_values"],
            input_ids=inputs_base["input_ids"],
            attention_mask=inputs_base["attention_mask"],
            max_new_tokens=max_new_tokens
        )

        answer_base = base_processor.tokenizer.decode(
            generated_tokens_base, skip_special_tokens=True
        ).strip()

        # --- Fine-Tuned Model Inference ---
        inputs_ft = fine_tuned_processor(
            text=prompt,
            images=img,
            return_tensors="pt",
            padding=True,
            truncation=True
        ).to(fine_tuned_model.device)

        generated_tokens_ft = fine_tuned_model.generate(
            processor = fine_tuned_processor,
            pixel_values=inputs_ft["pixel_values"],
            input_ids=inputs_ft["input_ids"],
            attention_mask=inputs_ft["attention_mask"],
            max_new_tokens=max_new_tokens
        )

        answer_ft = fine_tuned_processor.tokenizer.decode(
            generated_tokens_ft, skip_special_tokens=True
        ).strip()

        # --- Display Results ---
        print("═" * 80)
        print(f"Sample {idx}")
        print("Prompt:", question.replace("<image>", "[IMAGE]"))
        print("\nGround-truth:", gt_answer)
        print("Model Answer Before LORA:", answer_base or "<empty>")
        print("Fine-Tuned Model Answer:", answer_ft or "<empty>")
        display(img)

# Run with one sample, greedy decoding
seed = 19
test_ds = load_dataset(
    "unsloth/llava-instruct-mix-vsft-mini",
    split="test",
    streaming=False
).shuffle(seed=seed)

# Run comparison
compare_infer_results(
    ds=test_ds,
    base_model=base_model,
    base_processor=base_processor,
    fine_tuned_model=model,
    fine_tuned_processor=processor,
    idxes=list(range(8)),
    max_new_tokens=128,
    do_sample=False  # Greedy decoding for consistency
)

        # with torch.no_grad():
            # inputs_embeds_ft, new_attention_mask_ft, _ = fine_tuned_model.token_mixer(
            #     pixel_values=inputs_ft["pixel_values"],
            #     input_ids=inputs_ft["input_ids"],
            #     attention_mask=inputs_ft["attention_mask"],
            #     labels=None,
            #     tokenizer=fine_tuned_processor.tokenizer,
            #     language_model_embedding=fine_tuned_model.get_input_embeddings(),
            #     vision_tower=fine_tuned_model.vision_tower,
            #     projector=fine_tuned_model.multi_modal_projector,
            #     llama_hidden_size=2048
            # )


        # generated_tokens_ft = []
        # current_embeds_ft = inputs_embeds_ft
        # current_attention_mask_ft = new_attention_mask_ft
        # past_key_values_ft = None

        # for step in range(max_new_tokens):
        #     with torch.no_grad():
        #         outputs_ft = fine_tuned_model(
        #             inputs_embeds=current_embeds_ft[:, -1:, :] if step > 0 else current_embeds_ft,
        #             attention_mask=current_attention_mask_ft,
        #             past_key_values=past_key_values_ft,
        #             use_cache=True
        #         )
        #         logits_ft = outputs_ft.logits
        #         past_key_values_ft = outputs_ft.past_key_values

        #     next_token_logits_ft = logits_ft[:, -1, :]
        #     if do_sample:
        #         scaled_logits = next_token_logits_ft / temperature
        #         probs = F.softmax(scaled_logits, dim=-1)
        #         sorted_probs, sorted_indices = torch.sort(probs, descending=True)
        #         cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
        #         sorted_indices_to_remove = cumulative_probs > top_p
        #         sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
        #         sorted_indices_to_remove[..., 0] = False
        #         probs[sorted_indices_to_remove] = 0
        #         probs = probs / probs.sum(dim=-1, keepdim=True)
        #         next_token = torch.multinomial(probs, num_samples=1)
        #     else:
        #         next_token = torch.argmax(next_token_logits_ft, dim=-1, keepdim=True)

        #     generated_tokens_ft.append(next_token.item())
        #     if next_token.item() == fine_tuned_processor.tokenizer.eos_token_id:
        #         break

        #     next_token_embed = fine_tuned_model.get_input_embeddings()(next_token)
        #     current_embeds_ft = torch.cat([current_embeds_ft, next_token_embed], dim=1)
        #     current_attention_mask_ft = torch.cat(
        #         [current_attention_mask_ft, torch.ones_like(next_token, dtype=torch.long, device=fine_tuned_model.device)],
        #         dim=1
        #     )



from google.colab import drive
drive.mount('/content/drive')

!mkdir -p "/content/drive/MyDrive/tinyllava-threshold"

!cp -r "/content/tinyllava-lora/{today_dir}" "/content/drive/MyDrive/tinyllava-threshold"

!cp -r /content/tinyllava-lora/checkpoint-6000/ "/content/drive/MyDrive/{today_dir}"